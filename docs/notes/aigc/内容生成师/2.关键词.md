---
title: 关键词
createTime: 2025/09/11 22:23:37
permalink: /aigc/5f0hrjwa/
---
GPT全称（Generative Pre-trained Transformer)，生成式预训练模型

预训练大模型在AIGC中主要作用是==加速训练过程=={.tip}和==提高模型性能===={.important}

## 自然语言理解

### NLP

> NLP 被称作人工智能皇冠上的明珠，由于语义理解需要海量数据让AI 理解常识而壁垒较高。

- 机器翻译：将一种语言的文本翻译成另一种语言。
- 文本分类：将文本分为不同的类别，如垃圾邮件过滤、情感分析等。
- 信息检索：通过搜索引擎检索相关文档或网页。
- 文本生成：生成文章、摘要、对话等自然语言文本。
- 语音识别：将语音转换为文本，用于语音助手和语音命令。
- 问答系统：回答用户提出的自然语言问题，如智能助手和聊天机器人。
- 自动摘要：从长文本中提取关键信息以创建摘要。
- 命名实体识别：识别文本中的人名、地名、组织名等特定实体。

### LLM

LLM(Large Lauguage Model)大语言模型，用于提高基础语言理解和生成能力、支持==模型微调=={.tip}进一步优化应对特定任务

## 使用模型

### 补全模型（早期）

定义：基于给定文本生成连贯内容的模型。

特点：专注于理解和延续已有的文本信息，

例子：早期的GPT-2，擅长根据文本片段生成连续文本。

### 对话模型（现在）

定义：为==交互式=={.note}对话设计，能处理多轮对话和上下文信息的模型。

特点：更加关注对话的动态性和交互性，能维持话题连贯性。

例子：GPT-3和GPT-4，适用于复杂的对话系统。

#### Transformer训练

Transformer模型的核心思想是==自注意力机制=={.warning}，它能够有效地捕捉输入序列中不同位置之间的关系，而无需依赖循环神经网络（RNN）或卷积神经网络（CNN）等传统的序列处理方法。

NLP 领域效果较好的深度学习模型

Transformer 最大的特点是，可以让每个单元都可以捕捉到整句的信息，使用自注意力（self-attention）机制，这使得模型能够有效地处理距离依赖问题，并且在并行处理方面表现出色.

#### RLHF训练

RLHFiZ (Reinforcement Learning from Human Feedback)

• 通过有人类反馈的加强学习（RLHF）训练，生成式AI能够更好地理解用户指令。

## 大模型与小模型的比较

- ﻿参数量：大模型的参数量远远超过小模型，例如GPT-3拥有1,750亿个参数，相比传统深度学习小模型至少大了一万倍。
- ﻿训练方法：大模型通常采用无监督训练（NLP，transfomer 的引入），而小模型可能需要常规的调参过程。
- ﻿﻿通用性：大规模预训练模型（如GPT-3）不再特定于某一任务，而是对多个任务都有较好的效果，而小模型可能需要为不同的任务使用不同的模型进行单独训练。
- ﻿数据需求：大模型首先使用海量数据进行预训练，然后用这套参数对模型进行初始化，这一过程大幅降低了后续对数据量的需求。